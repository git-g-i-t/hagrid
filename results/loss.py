import os
import glob
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

EXPERIMENTS = {
    "Baseline":        "hagrid_v2/work_dir/ResNet18_base/logs/train",
    "Pretrained":      "hagrid_v2/work_dir/ResNet18_pre/logs/train",
    "SE-ResNet18":     "hagrid_v2/work_dir/SE_ResNet18_Attention/logs/train",
    "CBAM-ResNet18":   "hagrid_v2/work_dir/CBAM_ResNet18_Attention/logs/train",
    "Coord-ResNet18":  "hagrid_v2/work_dir/Coord_ResNet18/logs/train",
}

TAG_TO_PLOT = "loss/Train" 
SMOOTH_FACTOR = 0.6  # ç¨å¾®å¹³æ»‘ï¼Œä¿ç•™ä¸€å®šçš„æ³¢åŠ¨ç»†èŠ‚
SAVE_NAME = "results/loss_convergence_white.png"

# ================= ğŸ”§ å·¥å…·å‡½æ•° =================

def smooth(scalars, weight):
    if weight <= 0: return scalars
    last = scalars[0]
    smoothed = list()
    for point in scalars:
        smoothed_val = last * weight + (1 - weight) * point
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed

def read_tensorboard_data(log_dir, tag):
    event_files = glob.glob(os.path.join(log_dir, "events.out.tfevents*"))
    if not event_files:
        print(f"âŒ é”™è¯¯: {log_dir} æ²¡æ‰¾åˆ°æ—¥å¿—")
        return None, None
    event_file = max(event_files, key=os.path.getctime)
    ea = EventAccumulator(event_file)
    ea.Reload()
    if tag not in ea.Tags()['scalars']:
        return None, None
    events = ea.Scalars(tag)
    return [x.step for x in events], [x.value for x in events]

# ================= ğŸ¨ ç»˜å›¾ä¸»é€»è¾‘ =================

def main():
    # 1. è®¾ç½®ç™½è‰²èƒŒæ™¯é£æ ¼
    plt.rcParams['figure.facecolor'] = 'white'
    plt.rcParams['axes.facecolor'] = 'white'
    
    fig, ax = plt.subplots(figsize=(11, 6), dpi=150)
    
    # é¢œè‰²å’Œæ ‡è®°ç¬¦å·
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    markers = ['o', 's', '^', 'D', 'v'] # åœ†å½¢ã€æ­£æ–¹å½¢ã€ä¸Šä¸‰è§’ã€è±å½¢ã€ä¸‹ä¸‰è§’

    for i, (label, log_dir) in enumerate(EXPERIMENTS.items()):
        steps, values = read_tensorboard_data(log_dir, TAG_TO_PLOT)
        if steps is None: continue

        # å¹³æ»‘å¤„ç†
        plot_values = smooth(values, SMOOTH_FACTOR)
        
        # å¯»æ‰¾æœ€å°å€¼ç‚¹
        min_val = min(plot_values)
        min_idx = np.argmin(plot_values)
        min_step = steps[min_idx]

        color = colors[i % len(colors)]
        marker = markers[i % len(markers)]

        # ç»˜åˆ¶æŠ˜çº¿
        # markevery: æ¯éš”å‡ ä¸ªç‚¹ç”»ä¸€ä¸ªå½¢çŠ¶ï¼Œé˜²æ­¢ç‚¹å¤ªå¯†é›†
        ax.plot(steps, plot_values, label=label, color=color, linewidth=2,
                marker=marker, markersize=7, markevery=max(1, len(steps)//15),
                markerfacecolor=color, markeredgecolor='white', markeredgewidth=1)

        # ä»…æ ‡æ³¨æœ€å°å€¼
        # xytext ç¨å¾®å‘å³åç§»ä¸€ç‚¹ï¼Œé˜²æ­¢é®æŒ¡æŠ˜çº¿
        ax.annotate(f"{min_val:.3f}", 
                    xy=(min_step, min_val), 
                    xytext=(5, 2), 
                    textcoords='offset points',
                    fontsize=10, 
                    color=color, 
                    fontweight='bold')

    # å›¾è¡¨ç»†èŠ‚è£…é¥°
    ax.set_title("Training Loss Convergence", fontsize=16, fontweight='bold', pad=15)
    ax.set_xlabel("Epochs", fontsize=13)
    ax.set_ylabel("Train", fontsize=13)
    
    # è®¾ç½®ç½‘æ ¼ä¸ºç°è‰²è™šçº¿
    ax.grid(True, linestyle='--', color='lightgray', alpha=0.8)
    
    # å»æ‰ä¸Šæ–¹å’Œå³æ–¹çš„è¾¹æ¡†ï¼ˆå¯é€‰ï¼Œè®©ç”»é¢æ›´å¼€é˜”ï¼‰
    # ax.spines['top'].set_visible(False)
    # ax.spines['right'].set_visible(False)

    ax.legend(loc='upper right', fontsize=11, frameon=True, shadow=False)

    plt.tight_layout()
    os.makedirs(os.path.dirname(SAVE_NAME), exist_ok=True)
    plt.savefig(SAVE_NAME, bbox_inches='tight')
    print(f"\nâœ… ç»˜å›¾å®Œæˆï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {SAVE_NAME}")
    plt.show()

if __name__ == "__main__":
    main()