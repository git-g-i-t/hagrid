import torch
import matplotlib.pyplot as plt
from omegaconf import OmegaConf

from custom_utils.train_utils import (
    load_train_objects,
    load_train_optimizer,
    Trainer
)
from custom_utils.utils import (
    F1ScoreWithLogging,
    set_random_seed
)

from models.classifiers.se_resnet import SEResNet, SEBasicBlock
from models.classifiers.base_model_my import ClassifierModel

# ================= é…ç½®åŒºåŸŸ =================
CONFIG_PATH = "hagrid_v2/configs/se_resnet18.yaml"
SEARCH_EPOCHS = 25
GPU_ID = 0

# æœç´¢ç©ºé—´ï¼šåˆ†ç±»å¤´éšè—å±‚ç»“æ„
SEARCH_SPACE = {
    #"0-Layer (Standard)": [],
    #"1-Layer (Hidden 512)": [512],
    # "1-Layer (Hidden 256)": [256],
    # "2-Layers (512->256)": [512, 256],
    # "3-Layers (512->256->128)": [512, 256, 128]
}
# ===========================================


def run_search():
    set_random_seed(42)

    conf = OmegaConf.load(CONFIG_PATH)

    # æœç´¢é˜¶æ®µï¼šçŸ­å‘¨æœŸè®­ç»ƒ
    conf.epochs = SEARCH_EPOCHS
    conf.model.pretrained = False  # æ˜ç¡®ä» 0 å¼€å§‹è®­ç»ƒ

    # 1. åŠ è½½æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, _ = load_train_objects(
        conf, "train", n_gpu=1
    )

    results = {
        "0-Layer (Standard)": 0.83,
        "1-Layer (512)": 0.612,
        "1-Layer (256)": 0.556,
        "2-Layers": 0.667,
        "3-Layers": 0.500
    }

    # 2. ç»“æ„æœç´¢
    for name, hidden_layers in SEARCH_SPACE.items():
        print(f"\nğŸš€ æµ‹è¯•ç»“æ„: {name} | hidden_layers={hidden_layers}")

        # ---------- æ„å»ºæ¨¡å‹ ----------
        backbone = SEResNet(
            SEBasicBlock,
            [2, 2, 2, 2],
            num_classes=len(conf.dataset.targets),
            hidden_layers=hidden_layers,
        )

        model_wrapper = ClassifierModel(
            lambda **k: backbone,
            num_classes=len(conf.dataset.targets),
        )
        model_wrapper.type = "classifier"
        model_wrapper.criterion = getattr(torch.nn, conf.criterion)()

        # ---------- ä¼˜åŒ–å™¨ & æŒ‡æ ‡ ----------
        optimizer, scheduler = load_train_optimizer(model_wrapper, conf)
        metric = F1ScoreWithLogging(
            task="multiclass",
            num_classes=len(conf.dataset.targets),
        )

        # é˜²æ­¢å®éªŒæ—¥å¿—è¦†ç›–
        safe_name = name.replace(" ", "_").replace("(", "").replace(")", "")
        conf.experiment_name = f"Search_{safe_name}"

        trainer = Trainer(
            model=model_wrapper,
            config=conf,
            optimizer=optimizer,
            scheduler=scheduler,
            metric_calculator=metric,
            train_data=train_loader,
            val_data=val_loader,
            test_data=test_loader,
            n_gpu=1,
        )

        # ---------- å¼€å§‹è®­ç»ƒ ----------
        trainer.train()

        # ============================================================
        # âœ… ç¨³å®šæ€§è¯„ä»·æŒ‡æ ‡ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
        # å–æœ€å K ä¸ª epoch çš„å¹³å‡ F1ï¼Œè€Œä¸æ˜¯ max
        # ============================================================
        K = 5

        if hasattr(trainer, "val_f1_history"):
            val_f1s = trainer.val_f1_history
        elif hasattr(trainer, "history") and \
             "val" in trainer.history and \
             "F1Score" in trainer.history["val"]:
            val_f1s = trainer.history["val"]["F1Score"]
        else:
            raise RuntimeError(
                "âŒ Trainer ä¸­æœªæ‰¾åˆ°éªŒè¯ F1 å†å²ï¼Œè¯·ä¿å­˜ val F1 è®°å½•"
            )

        if len(val_f1s) >= K:
            stable_f1 = sum(val_f1s[-K:]) / K
        else:
            stable_f1 = sum(val_f1s) / len(val_f1s)

        results[name] = stable_f1
        print(f"âœ… {name} ç¨³å®š F1 (last {K} epochs): {stable_f1:.4f}")

    # 3. ç”»å›¾æ€»ç»“
    plot_results(results)


def plot_results(results):
    plt.figure(figsize=(8, 5))
    names = list(results.keys())
    scores = list(results.values())

    plt.bar(names, scores)
    plt.title(f"MLP Depth Search (Stable Metric, Epochs={SEARCH_EPOCHS})")
    plt.ylabel("Stable Validation F1")
    plt.ylim(0, 1.0)
    plt.grid(axis="y", linestyle="--", alpha=0.6)

    for i, v in enumerate(scores):
        plt.text(i, v + 0.01, f"{v:.3f}", ha="center", fontweight="bold")

    plt.tight_layout()
    plt.savefig("results/mlp_depth_search_stable.png")
    print("\nğŸ“Š æœç´¢ç»“æœå·²ä¿å­˜ä¸º results/mlp_depth_search_stable.png")
    plt.show()


if __name__ == "__main__":
    run_search()
